"""
Hospital Readmission Prediction - Fairness Auditing
====================================================
Comprehensive bias detection and fairness metrics across demographic groups.
"""

import numpy as np
import pandas as pd
from sklearn.metrics import confusion_matrix, recall_score, precision_score
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List

class FairnessAuditor:
    """
    Comprehensive fairness auditing for healthcare AI models.
    Implements multiple fairness metrics and bias detection.
    """
    
    def __init__(self):
        self.audit_results = {}
        
    def audit_model_fairness(self, model, X_test, y_test, 
                            sensitive_features: Dict[str, pd.Series],
                            threshold=0.5):
        """
        Perform comprehensive fairness audit across demographic groups.
        
        Parameters:
        -----------
        model : trained model
            Model with predict_proba method
        X_test : pd.DataFrame
            Test features
        y_test : pd.Series
            True labels
        sensitive_features : dict
            Dictionary of sensitive attributes, e.g., 
            {'race': race_series, 'gender': gender_series, 'age_group': age_series}
        threshold : float
            Classification threshold
            
        Returns:
        --------
        dict : Comprehensive fairness metrics
        """
        print("=" * 70)
        print("FAIRNESS AUDIT REPORT")
        print("=" * 70)
        
        # Get predictions
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        y_pred = (y_pred_proba >= threshold).astype(int)
        
        # Overall metrics
        print("\n### OVERALL PERFORMANCE ###\n")
        overall_metrics = self._calculate_metrics(y_test, y_pred, y_pred_proba)
        self._print_metrics(overall_metrics, "Overall")
        
        # Analyze each sensitive attribute
        fairness_report = {'overall': overall_metrics}
        
        for attr_name, attr_values in sensitive_features.items():
            print(f"\n{'=' * 70}")
            print(f"### FAIRNESS ANALYSIS: {attr_name.upper()} ###")
            print("=" * 70)
            
            group_metrics = self._analyze_by_group(
                y_test, y_pred, y_pred_proba, attr_values, attr_name
            )
            
            fairness_report[attr_name] = group_metrics
            
            # Calculate disparity metrics
            disparities = self._calculate_disparities(group_metrics)
            fairness_report[f'{attr_name}_disparities'] = disparities
            
            print(f"\n--- Disparity Summary for {attr_name} ---")
            for metric, disparity in disparities.items():
                status = "✓ PASS" if disparity < 0.05 else "⚠ WARNING" if disparity < 0.10 else "✗ FAIL"
                print(f"{metric}: {disparity:.3f} {status}")
        
        self.audit_results = fairness_report
        
        return fairness_report
    
    def _calculate_metrics(self, y_true, y_pred, y_pred_proba):
        """Calculate comprehensive performance metrics."""
        from sklearn.metrics import roc_auc_score, accuracy_score
        
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        # Calculate metrics with zero-division handling
        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # Sensitivity/Recall
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
        tnr = tn / (tn + fp) if (tn + fp) > 0 else 0  # Specificity
        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        npv = tn / (tn + fn) if (tn + fn) > 0 else 0
        
        accuracy = accuracy_score(y_true, y_pred)
        
        try:
            auc = roc_auc_score(y_true, y_pred_proba)
        except:
            auc = 0.0
        
        return {
            'sample_size': len(y_true),
            'prevalence': y_true.mean(),
            'positive_rate': y_pred.mean(),  # Selection rate
            'accuracy': accuracy,
            'auc': auc,
            'sensitivity': tpr,  # TPR
            'specificity': tnr,
            'precision': precision,  # PPV
            'npv': npv,
            'fpr': fpr,
            'fnr': fnr,
            'tp': tp,
            'fp': fp,
            'tn': tn,
            'fn': fn
        }
    
    def _analyze_by_group(self, y_true, y_pred, y_pred_proba, group_labels, group_name):
        """Analyze performance metrics by demographic group."""
        unique_groups = group_labels.unique()
        group_metrics = {}
        
        print(f"\nPerformance by {group_name}:")
        print("-" * 70)
        
        for group in sorted(unique_groups):
            mask = (group_labels == group).values
            
            if mask.sum() < 10:  # Skip groups with too few samples
                print(f"\n{group}: Insufficient data (n={mask.sum()})")
                continue
            
            metrics = self._calculate_metrics(
                y_true[mask],
                y_pred[mask],
                y_pred_proba[mask]
            )
            
            group_metrics[group] = metrics
            self._print_metrics(metrics, str(group))
        
        return group_metrics
    
    def _print_metrics(self, metrics, label):
        """Pretty print metrics."""
        print(f"\n{label}:")
        print(f"  Sample Size: {metrics['sample_size']}")
        print(f"  Prevalence (actual readmission rate): {metrics['prevalence']:.1%}")
        print(f"  Selection Rate (predicted high-risk): {metrics['positive_rate']:.1%}")
        print(f"  AUC: {metrics['auc']:.4f}")
        print(f"  Accuracy: {metrics['accuracy']:.4f}")
        print(f"  Sensitivity (Recall/TPR): {metrics['sensitivity']:.4f}")
        print(f"  Specificity (TNR): {metrics['specificity']:.4f}")
        print(f"  Precision (PPV): {metrics['precision']:.4f}")
        print(f"  FPR: {metrics['fpr']:.4f}")
        print(f"  FNR: {metrics['fnr']:.4f}")
    
    def _calculate_disparities(self, group_metrics):
        """
        Calculate disparity metrics between demographic groups.
        Uses max difference approach.
        """
        if len(group_metrics) < 2:
            return {}
        
        # Extract key metrics for each group
        metrics_by_group = {
            group: {
                'sensitivity': data['sensitivity'],
                'fpr': data['fpr'],
                'precision': data['precision'],
                'selection_rate': data['positive_rate']
            }
            for group, data in group_metrics.items()
        }
        
        # Calculate max disparity for each metric
        disparities = {}
        
        for metric in ['sensitivity', 'fpr', 'precision', 'selection_rate']:
            values = [data[metric] for data in metrics_by_group.values()]
            max_disparity = max(values) - min(values)
            disparities[f'{metric}_disparity'] = max_disparity
        
        return disparities
    
    def check_fairness_criteria(self, disparity_threshold=0.05):
        """
        Check if model meets fairness criteria.
        
        Parameters:
        -----------
        disparity_threshold : float
            Maximum acceptable disparity (default: 5%)
            
        Returns:
        --------
        dict : Pass/fail status for each fairness criterion
        """
        if not self.audit_results:
            print("No audit results available. Run audit_model_fairness first.")
            return {}
        
        print("\n" + "=" * 70)
        print("FAIRNESS CRITERIA EVALUATION")
        print("=" * 70)
        print(f"Threshold: {disparity_threshold*100}% maximum disparity\n")
        
        fairness_status = {}
        
        for attr_name in self.audit_results:
            if attr_name == 'overall' or not attr_name.endswith('_disparities'):
                continue
            
            base_attr = attr_name.replace('_disparities', '')
            disparities = self.audit_results[attr_name]
            
            print(f"\n{base_attr.upper()}:")
            attr_pass = True
            
            for metric, value in disparities.items():
                passed = value <= disparity_threshold
                attr_pass = attr_pass and passed
                status_icon = "✓" if passed else "✗"
                
                print(f"  {status_icon} {metric}: {value:.3f} ({'PASS' if passed else 'FAIL'})")
            
            fairness_status[base_attr] = attr_pass
            
            overall_status = "✓ FAIR" if attr_pass else "✗ UNFAIR"
            print(f"\n  Overall: {overall_status}")
        
        return fairness_status
    
    def visualize_fairness(self, sensitive_attribute='race'):
        """Create visualizations of fairness metrics."""
        if sensitive_attribute not in self.audit_results:
            print(f"No data for attribute: {sensitive_attribute}")
            return
        
        group_metrics = self.audit_results[sensitive_attribute]
        
        # Prepare data for plotting
        groups = list(group_metrics.keys())
        sensitivity = [group_metrics[g]['sensitivity'] for g in groups]
        precision = [group_metrics[g]['precision'] for g in groups]
        fpr = [group_metrics[g]['fpr'] for g in groups]
        
        # Create subplots
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        # Sensitivity comparison
        axes[0].bar(groups, sensitivity, color='steelblue', alpha=0.7)
        axes[0].axhline(y=0.80, color='red', linestyle='--', label='Target (80%)')
        axes[0].set_ylabel('Sensitivity (Recall)')
        axes[0].set_title(f'Sensitivity by {sensitive_attribute.title()}')
        axes[0].set_ylim([0, 1])
        axes[0].legend()
        axes[0].tick_params(axis='x', rotation=45)
        
        # Precision comparison
        axes[1].bar(groups, precision, color='forestgreen', alpha=0.7)
        axes[1].set_ylabel('Precision (PPV)')
        axes[1].set_title(f'Precision by {sensitive_attribute.title()}')
        axes[1].set_ylim([0, 1])
        axes[1].tick_params(axis='x', rotation=45)
        
        # FPR comparison
        axes[2].bar(groups, fpr, color='coral', alpha=0.7)
        axes[2].set_ylabel('False Positive Rate')
        axes[2].set_title(f'FPR by {sensitive_attribute.title()}')
        axes[2].set_ylim([0, 0.5])
        axes[2].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.show()
    
    def generate_fairness_report(self, output_file='fairness_report.txt'):
        """Generate comprehensive fairness report."""
        if not self.audit_results:
            print("No audit results available.")
            return
        
        with open(output_file, 'w') as f:
            f.write("=" * 70 + "\n")
            f.write("HOSPITAL READMISSION MODEL - FAIRNESS AUDIT REPORT\n")
            f.write("=" * 70 + "\n\n")
            
            # Overall performance
            f.write("OVERALL PERFORMANCE\n")
            f.write("-" * 70 + "\n")
            overall = self.audit_results['overall']
            for metric, value in overall.items():
                if isinstance(value, float):
                    f.write(f"{metric}: {value:.4f}\n")
                else:
                    f.write(f"{metric}: {value}\n")
            
            # Group-level analysis
            for attr_name in self.audit_results:
                if attr_name == 'overall' or attr_name.endswith('_disparities'):
                    continue
                
                f.write(f"\n\n{attr_name.upper()} ANALYSIS\n")
                f.write("-" * 70 + "\n")
                
                group_data = self.audit_results[attr_name]
                for group, metrics in group_data.items():
                    f.write(f"\n{group}:\n")
                    for metric, value in metrics.items():
                        if isinstance(value, float):
                            f.write(f"  {metric}: {value:.4f}\n")
                        else:
                            f.write(f"  {metric}: {value}\n")
                
                # Disparities
                if f'{attr_name}_disparities' in self.audit_results:
                    f.write(f"\nDisparities:\n")
                    disparities = self.audit_results[f'{attr_name}_disparities']
                    for metric, value in disparities.items():
                        f.write(f"  {metric}: {value:.4f}\n")
        
        print(f"\nFairness report saved to: {output_file}")


# Example usage
if __name__ == "__main__":
    # Generate synthetic test data
    np.random.seed(42)
    n_test = 1000
    
    # Create synthetic features
    X_test = pd.DataFrame(np.random.randn(n_test, 20))
    
    # Create synthetic demographic data
    race = np.random.choice(['White', 'Black', 'Hispanic', 'Asian'], 
                           n_test, p=[0.60, 0.20, 0.15, 0.05])
    gender = np.random.choice(['M', 'F'], n_test, p=[0.48, 0.52])
    age_group = np.random.choice(['18-40', '41-65', '66-80', '81+'], 
                                n_test, p=[0.15, 0.35, 0.35, 0.15])
    
    # Create outcome with some bias
    y_test = np.random.binomial(1, 0.15, n_test)
    
    # Create a simple mock model
    class MockModel:
        def predict_proba(self, X):
            # Simulate predictions with some bias
            proba_0 = np.random.random(len(X)) * 0.7 + 0.15
            proba_1 = 1 - proba_0
            return np.column_stack([proba_0, proba_1])
    
    model = MockModel()
    
    # Create fairness auditor
    auditor = FairnessAuditor()
    
    # Perform fairness audit
    sensitive_features = {
        'race': pd.Series(race),
        'gender': pd.Series(gender),
        'age_group': pd.Series(age_group)
    }
    
    audit_results = auditor.audit_model_fairness(
        model, X_test, y_test, sensitive_features, threshold=0.5
    )
    
    # Check fairness criteria
    fairness_status = auditor.check_fairness_criteria(disparity_threshold=0.05)
    
    # Generate report
    auditor.generate_fairness_report('fairness_report.txt')
    
    print("\n✓ Fairness audit complete!")
